\documentclass{report}

% CS231n: Convolutional Neural Networks for Visual Recognition

% web address: http://cs231n.github.io/
\usepackage{graphicx}
\usepackage{amsmath}

\begin{document}
	
	\section{Image Classification}
	
	\subsection{Motivation}
	
	In this section we will introduce the Image Classification problem, which is the task of assigning an input image one label from a fixed set of categories. This is one of the core problems in Computer Vision that, despite its simplicity, has a large variety of practical applications. 
	
	\subsection{Validation sets for Hyperparameter tuning}
	
	Luckily, there is a correct way of tuning the hyperparameters and it does not touch the test set at all. The idea is to split our training set in two: a slightly smaller training set, and what we call a \textbf{validation set}. This validation set is essentially used as a fake test set to tune the hyper-parameters.
	
	
	\subsection{Cross-validation}
	
	In cases where the size of your training data (and therefore also the validation data) might be small, people sometimes use a more sophisticated technique for hyperparameter tuning called \textbf{cross-validation}. 
	
	
	\subsection{Pros and Cons of Nearest Neighbor classifier}
	
	In practice we often care about the test time efficiency much more than the efficiency at training time. In fact, the deep neural networks we will develop later in this class shift this tradeoff to the other extreme: They are very expensive to train, but once the training is finished it is very cheap to classify a new test example. This mode of operation is much more desirable in practice.
	
	\newpage
	
	\section{Linear Classification}
	
	\subsection{Interpreting a linear classifier}
	
	\begin{figure}[h]
		\includegraphics[width=\textwidth]{imagemap.jpg}
	\end{figure}
	
	\textbf{Bias trick.} Before moving on we want to mention a common simplifying trick to representing the two parameters W,b as one. \\
	
	\begin{figure}[h]
		\includegraphics[width=0.6\textheight]{wb.jpeg}
	\end{figure}


	\textbf{Image data preprocessing.} As a quick note, in the examples above we used the raw pixel values (which range from [0…255]). In Machine Learning, it is a very common practice to always perform normalization of your input features (in the case of images, every pixel is thought of as a feature). In particular, it is important to \textbf{center your data} by subtracting the mean from every feature. In the case of images, this corresponds to computing a mean image across the training images and subtracting it from every image to get images where the pixels range from approximately [-127 … 127]. Further common preprocessing is to scale each input feature so that its values range from [-1, 1].
	
	\subsection{Loss function}

	We fed in the pixels that depict a cat but the cat score came out very low (-96.8) compared to the other classes (dog score 437.9 and ship score 61.95). We are going to measure our unhappiness with outcomes such as this one with a loss function (or sometimes also referred to as the cost function or the objective). Intuitively, the loss will be high if we’re doing a poor job of classifying the training data, and it will be low if we’re doing well.

	\subsubsection{Multiclass Support Vector Machine loss}

	There are several ways to define the details of the loss function. As a first example we will first develop a commonly used loss called the Multiclass Support Vector Machine (SVM) loss. The SVM loss is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin $\Delta$. Notice that it’s sometimes helpful to anthropomorphise the loss functions as we did above: The SVM “wants” a certain outcome in the sense that the outcome would yield a lower loss (which is good). \\
	
	A last piece of terminology we’ll mention before we finish with this section is that the threshold at zero max(0,-) function is often called the \textbf{hinge loss}.
	
	\begin{figure}
		\includegraphics[width=\textwidth]{margin.jpg}
	\end{figure}


	\textbf{Regularization.} In other words, we wish to encode some preference for a certain set of weights W over others to remove this ambiguity. We can do so by extending the loss function with a \textbf{regularization penalty R(W)}. The most common regularization penalty is the \textbf{L2} norm that discourages large weights through an elementwise quadratic penalty over all parameters:\\
	
	Notice that the regularization function is not a function of the data, it is only based on the weights. Including the regularization penalty completes the full Multiclass Support Vector Machine loss, which is made up of two components: the \textbf{data loss} (which is the average loss $L_i$ over all examples) and the \textbf{regularization loss}. That is, the full Multiclass SVM loss becomes:\\


	\begin{equation}
	L = \underbrace{\frac{1}{N}\sum_{i}^{}}_\text{data loss} + \underbrace{\lambda R(W)}_\text{regularization loss}
	\end{equation}














	
\end{document}

